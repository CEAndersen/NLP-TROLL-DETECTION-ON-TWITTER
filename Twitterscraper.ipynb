{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twitterscraper import query_tweets\n",
    "from twitterscraper import user\n",
    "import datetime as dt \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: queries: ['US election since:2016-05-20 until:2016-05-31', 'US election since:2016-05-31 until:2016-06-11', 'US election since:2016-06-11 until:2016-06-22', 'US election since:2016-06-22 until:2016-07-03', 'US election since:2016-07-03 until:2016-07-15', 'US election since:2016-07-15 until:2016-07-26', 'US election since:2016-07-26 until:2016-08-06', 'US election since:2016-08-06 until:2016-08-17', 'US election since:2016-08-17 until:2016-08-28', 'US election since:2016-08-28 until:2016-09-09', 'US election since:2016-09-09 until:2016-09-20', 'US election since:2016-09-20 until:2016-10-01', 'US election since:2016-10-01 until:2016-10-12', 'US election since:2016-10-12 until:2016-10-23', 'US election since:2016-10-23 until:2016-11-04', 'US election since:2016-11-04 until:2016-11-15', 'US election since:2016-11-15 until:2016-11-26', 'US election since:2016-11-26 until:2016-12-07', 'US election since:2016-12-07 until:2016-12-18', 'US election since:2016-12-18 until:2016-12-30']\n",
      "INFO: Querying US election since:2016-05-20 until:2016-05-31\n",
      "INFO: Querying US election since:2016-06-11 until:2016-06-22\n",
      "INFO: Querying US election since:2016-05-31 until:2016-06-11\n",
      "INFO: Querying US election since:2016-07-03 until:2016-07-15\n",
      "INFO: Querying US election since:2016-07-26 until:2016-08-06\n",
      "INFO: Querying US election since:2016-06-22 until:2016-07-03\n",
      "INFO: Querying US election since:2016-09-09 until:2016-09-20\n",
      "INFO: Querying US election since:2016-09-20 until:2016-10-01\n",
      "INFO: Querying US election since:2016-08-17 until:2016-08-28\n",
      "INFO: Querying US election since:2016-08-28 until:2016-09-09\n",
      "INFO: Querying US election since:2016-08-06 until:2016-08-17\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=US%20election%20since%3A2016-05-20%20until%3A2016-05-31&l=english\n",
      "INFO: Querying US election since:2016-07-15 until:2016-07-26\n",
      "INFO: Querying US election since:2016-11-04 until:2016-11-15\n",
      "INFO: Querying US election since:2016-10-12 until:2016-10-23\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=US%20election%20since%3A2016-06-11%20until%3A2016-06-22&l=english\n",
      "INFO: Querying US election since:2016-12-07 until:2016-12-18\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=US%20election%20since%3A2016-05-31%20until%3A2016-06-11&l=english\n",
      "INFO: Querying US election since:2016-10-01 until:2016-10-12\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=US%20election%20since%3A2016-08-17%20until%3A2016-08-28&l=english\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=US%20election%20since%3A2016-06-22%20until%3A2016-07-03&l=english\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=US%20election%20since%3A2016-09-09%20until%3A2016-09-20&l=english\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=US%20election%20since%3A2016-07-15%20until%3A2016-07-26&l=english\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=US%20election%20since%3A2016-07-26%20until%3A2016-08-06&l=english\n",
      "INFO: Using proxy 202.61.49.52:48298\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=US%20election%20since%3A2016-08-28%20until%3A2016-09-09&l=english\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=US%20election%20since%3A2016-08-06%20until%3A2016-08-17&l=english\n",
      "INFO: Using proxy 202.61.49.52:48298\n",
      "INFO: Using proxy 202.61.49.52:48298\n",
      "INFO: Using proxy 202.61.49.52:48298\n",
      "INFO: Using proxy 202.61.49.52:48298\n",
      "INFO: Using proxy 202.61.49.52:48298\n",
      "INFO: Using proxy 202.61.49.52:48298\n",
      "INFO: Using proxy 202.61.49.52:48298\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=US%20election%20since%3A2016-07-03%20until%3A2016-07-15&l=english\n",
      "INFO: Using proxy 202.61.49.52:48298\n",
      "INFO: Using proxy 202.61.49.52:48298\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=US%20election%20since%3A2016-09-20%20until%3A2016-10-01&l=english\n",
      "INFO: Using proxy 202.61.49.52:48298\n",
      "INFO: Using proxy 202.61.49.52:48298\n",
      "INFO: Querying US election since:2016-11-15 until:2016-11-26\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=US%20election%20since%3A2016-11-15%20until%3A2016-11-26&l=english\n",
      "INFO: Querying US election since:2016-11-26 until:2016-12-07\n",
      "INFO: Using proxy 202.61.49.52:48298\n",
      "INFO: Querying US election since:2016-10-23 until:2016-11-04\n",
      "INFO: Querying US election since:2016-12-18 until:2016-12-30\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=US%20election%20since%3A2016-11-26%20until%3A2016-12-07&l=english\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=US%20election%20since%3A2016-10-23%20until%3A2016-11-04&l=english\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=US%20election%20since%3A2016-12-18%20until%3A2016-12-30&l=english\n",
      "INFO: Using proxy 202.61.49.52:48298\n",
      "INFO: Using proxy 202.61.49.52:48298\n",
      "INFO: Using proxy 202.61.49.52:48298\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=US%20election%20since%3A2016-10-01%20until%3A2016-10-12&l=english\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=US%20election%20since%3A2016-10-12%20until%3A2016-10-23&l=english\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=US%20election%20since%3A2016-11-04%20until%3A2016-11-15&l=english\n",
      "INFO: Scraping tweets from https://twitter.com/search?f=tweets&vertical=default&q=US%20election%20since%3A2016-12-07%20until%3A2016-12-18&l=english\n",
      "INFO: Using proxy 202.61.49.52:48298\n",
      "INFO: Using proxy 202.61.49.52:48298\n",
      "INFO: Using proxy 202.61.49.52:48298\n",
      "INFO: Using proxy 202.61.49.52:48298\n",
      "Process 'ForkPoolWorker-40' pid:33149 exited with 'signal 11 (SIGSEGV)'\n",
      "Process 'ForkPoolWorker-39' pid:33148 exited with 'signal 11 (SIGSEGV)'\n",
      "Process 'ForkPoolWorker-38' pid:33147 exited with 'signal 11 (SIGSEGV)'\n",
      "Process 'ForkPoolWorker-37' pid:33146 exited with 'signal 11 (SIGSEGV)'\n",
      "Process 'ForkPoolWorker-36' pid:33145 exited with 'signal 11 (SIGSEGV)'\n",
      "Process 'ForkPoolWorker-35' pid:33144 exited with 'signal 11 (SIGSEGV)'\n",
      "Process 'ForkPoolWorker-34' pid:33143 exited with 'signal 11 (SIGSEGV)'\n",
      "Process 'ForkPoolWorker-33' pid:33142 exited with 'signal 11 (SIGSEGV)'\n",
      "Process 'ForkPoolWorker-32' pid:33141 exited with 'signal 11 (SIGSEGV)'\n",
      "Process 'ForkPoolWorker-31' pid:33140 exited with 'signal 11 (SIGSEGV)'\n",
      "Process 'ForkPoolWorker-30' pid:33139 exited with 'signal 11 (SIGSEGV)'\n",
      "Process 'ForkPoolWorker-29' pid:33138 exited with 'signal 11 (SIGSEGV)'\n",
      "Process 'ForkPoolWorker-28' pid:33137 exited with 'signal 11 (SIGSEGV)'\n",
      "Process 'ForkPoolWorker-27' pid:33136 exited with 'signal 11 (SIGSEGV)'\n",
      "Process 'ForkPoolWorker-26' pid:33135 exited with 'signal 11 (SIGSEGV)'\n",
      "Process 'ForkPoolWorker-25' pid:33134 exited with 'signal 11 (SIGSEGV)'\n",
      "Process 'ForkPoolWorker-24' pid:33133 exited with 'signal 11 (SIGSEGV)'\n",
      "Process 'ForkPoolWorker-23' pid:33132 exited with 'signal 11 (SIGSEGV)'\n",
      "Process 'ForkPoolWorker-22' pid:33131 exited with 'signal 11 (SIGSEGV)'\n",
      "Process 'ForkPoolWorker-21' pid:33130 exited with 'signal 11 (SIGSEGV)'\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Traceback (most recent call last):\n  File \"/Users/Pia/anaconda3/lib/python3.7/site-packages/billiard/pool.py\", line 1267, in mark_as_worker_lost\n    human_status(exitcode)),\nbilliard.exceptions.WorkerLostError: Worker exited prematurely: signal 11 (SIGSEGV).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-a91ed72043a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#user = realDonaldTrump #Get only tweets from this person\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'US election'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbegindate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbegin_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menddate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/twitterscraper/query.py\u001b[0m in \u001b[0;36mquery_tweets\u001b[0;34m(query, limit, begindate, enddate, poolsize, lang)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'queries: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mnew_tweets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_tweets_once\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit_per_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m                 \u001b[0mall_tweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_tweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                 logger.info('Got {} tweets ({} new).'.format(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/billiard/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1962\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1964\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1966\u001b[0m     \u001b[0m__next__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m                    \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Traceback (most recent call last):\n  File \"/Users/Pia/anaconda3/lib/python3.7/site-packages/billiard/pool.py\", line 1267, in mark_as_worker_lost\n    human_status(exitcode)),\nbilliard.exceptions.WorkerLostError: Worker exited prematurely: signal 11 (SIGSEGV).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "begin_date = dt.date(2016,5,20) #Input date to scrape from here \n",
    "end_date = dt.date(2016,12,30) #Input date to scrape until here \n",
    "\n",
    "\n",
    "limit = 100 #Max amounts of tweets I want to scrape \n",
    "lang = 'english'\n",
    "#user = realDonaldTrump #Get only tweets from this person \n",
    "\n",
    "tweets = query_tweets('US election', begindate = begin_date, enddate = end_date, limit = limit, lang = lang)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-d37aba37dcf3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtweetsverified\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'US election'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbegindate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbegin_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menddate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "tweetsverified = user('US election', begindate = begin_date, enddate = end_date, limit = limit, lang = lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweetsverified' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-c56cf3deb23e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweetsverified\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'/Users/Pia/Desktop/NLP/LDA/NoTrollScrape.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tweetsverified' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.DataFrame(t.__dict__ for t in tweetsverified)\n",
    "\n",
    "print(df[0:100])\n",
    "\n",
    "df.to_csv(r'/Users/Pia/Desktop/NLP/LDA/NoTrollScrape.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209036\n"
     ]
    }
   ],
   "source": [
    "print(len(df.text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
